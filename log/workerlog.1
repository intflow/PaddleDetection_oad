/opt/conda/lib/python3.8/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(module.__version__) < minver:
/root/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:108: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  long_ = _make_signed(np.long)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:109: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  ulong = _make_unsigned(np.long)
I0513 07:03:32.711344 25417 tcp_utils.cc:107] Retry to connect to 127.0.1.1:50419 while the server is not yet listening.
I0513 07:03:35.711776 25417 tcp_utils.cc:130] Successfully connected to 127.0.1.1:50419
W0513 07:03:37.213222 25417 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 8.6, Driver API Version: 11.7, Runtime API Version: 11.7
W0513 07:03:37.216537 25417 gpu_resources.cc:91] device: 1, cuDNN Version: 8.4.
[2023-05-13 07:03:40,020] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 6, mp_group: [1],  sharding_group: [1], pp_group: [1], dp_group: [0, 1, 2, 3, 4, 5], check/clip group: [1]
loading annotations into memory...
Done (t=0.14s)
creating index...
index created!
W0513 07:03:58.092099 25417 reducer.cc:622] All parameters are involved in the backward pass. It is recommended to set find_unused_parameters to False to improve performance. However, if unused parameters appear in subsequent iterative training, then an error will occur. Please make it clear that in the subsequent training, there will be no parameters that are not used in the backward pass, and then set find_unused_parameters


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   squeeze_ad_func(paddle::experimental::Tensor const&, paddle::experimental::IntArrayBase<paddle::experimental::Tensor>)
1   paddle::experimental::squeeze_intermediate(paddle::experimental::Tensor const&, paddle::experimental::IntArrayBase<paddle::experimental::Tensor> const&)
2   paddle::experimental::PrepareData(paddle::experimental::Tensor const&, phi::TensorArgDef const&, paddle::experimental::TransformFlag const&)
3   paddle::experimental::TransformData(phi::DenseTensor*, phi::TensorArgDef const&, paddle::experimental::TransformFlag const&)
4   paddle::experimental::TransDataPlace(phi::DenseTensor const&, phi::Place)
5   paddle::framework::TensorCopySync(phi::DenseTensor const&, phi::Place const&, phi::DenseTensor*)
6   void paddle::memory::Copy<phi::Place, phi::Place>(phi::Place, void*, phi::Place, void const*, unsigned long, void*)
7   void paddle::memory::Copy<phi::GPUPlace, phi::GPUPinnedPlace>(phi::GPUPlace, void*, phi::GPUPinnedPlace, void const*, unsigned long, void*)
8   phi::backends::gpu::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1683929827 (unix time) try "date -d @1683929827" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x62ec) received by PID 25417 (TID 0x7fed07a55740) from PID 25324 ***]

/opt/conda/lib/python3.8/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(module.__version__) < minver:
/root/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:108: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  long_ = _make_signed(np.long)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:109: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  ulong = _make_unsigned(np.long)
I0513 07:17:54.255697 16468 tcp_utils.cc:107] Retry to connect to 127.0.1.1:41845 while the server is not yet listening.
I0513 07:17:57.256201 16468 tcp_utils.cc:130] Successfully connected to 127.0.1.1:41845
W0513 07:17:59.147848 16468 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 8.6, Driver API Version: 11.7, Runtime API Version: 11.7
W0513 07:17:59.150841 16468 gpu_resources.cc:91] device: 1, cuDNN Version: 8.4.
[2023-05-13 07:18:01,600] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 6, mp_group: [1],  sharding_group: [1], pp_group: [1], dp_group: [0, 1, 2, 3, 4, 5], check/clip group: [1]
loading annotations into memory...
Done (t=1.58s)
creating index...
index created!
W0513 07:18:55.246451 16468 reducer.cc:622] All parameters are involved in the backward pass. It is recommended to set find_unused_parameters to False to improve performance. However, if unused parameters appear in subsequent iterative training, then an error will occur. Please make it clear that in the subsequent training, there will be no parameters that are not used in the backward pass, and then set find_unused_parameters


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   squeeze_ad_func(paddle::experimental::Tensor const&, paddle::experimental::IntArrayBase<paddle::experimental::Tensor>)
1   paddle::experimental::squeeze_intermediate(paddle::experimental::Tensor const&, paddle::experimental::IntArrayBase<paddle::experimental::Tensor> const&)
2   paddle::experimental::PrepareData(paddle::experimental::Tensor const&, phi::TensorArgDef const&, paddle::experimental::TransformFlag const&)
3   paddle::experimental::TransformData(phi::DenseTensor*, phi::TensorArgDef const&, paddle::experimental::TransformFlag const&)
4   paddle::experimental::TransDataPlace(phi::DenseTensor const&, phi::Place)
5   paddle::framework::TensorCopySync(phi::DenseTensor const&, phi::Place const&, phi::DenseTensor*)
6   void paddle::memory::Copy<phi::Place, phi::Place>(phi::Place, void*, phi::Place, void const*, unsigned long, void*)
7   void paddle::memory::Copy<phi::GPUPlace, phi::GPUPinnedPlace>(phi::GPUPlace, void*, phi::GPUPinnedPlace, void const*, unsigned long, void*)
8   phi::backends::gpu::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1683936926 (unix time) try "date -d @1683936926" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x3ff5) received by PID 16468 (TID 0x7f53020e3740) from PID 16373 ***]

Exception in thread Thread-31:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 623, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File "/opt/conda/lib/python3.8/multiprocessing/queues.py", line 108, in get
/opt/conda/lib/python3.8/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(module.__version__) < minver:
/root/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:108: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  long_ = _make_signed(np.long)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:109: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  ulong = _make_unsigned(np.long)
usage: train.py [-h] [-c CONFIG] [-o [OPT [OPT ...]]] [--eval] [-r RESUME]
                [--slim_config SLIM_CONFIG] [--enable_ce ENABLE_CE] [--amp]
                [--fleet] [--use_vdl USE_VDL] [--vdl_log_dir VDL_LOG_DIR]
                [--use_wandb USE_WANDB] [--save_prediction_only]
                [--profiler_options PROFILER_OPTIONS] [--save_proposals]
                [--proposals_path PROPOSALS_PATH] [--to_static]
train.py: error: argument -r/--resume: expected one argument
/opt/conda/lib/python3.8/site-packages/matplotlib/__init__.py:169: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(module.__version__) < minver:
/root/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:108: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  long_ = _make_signed(np.long)
/opt/conda/lib/python3.8/site-packages/numba/core/types/__init__.py:109: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  ulong = _make_unsigned(np.long)
I0513 09:34:31.417356 25265 tcp_utils.cc:130] Successfully connected to 127.0.1.1:62178
W0513 09:34:35.826993 25265 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 8.6, Driver API Version: 11.7, Runtime API Version: 11.7
W0513 09:34:35.829819 25265 gpu_resources.cc:91] device: 1, cuDNN Version: 8.4.
[2023-05-13 09:34:38,188] [    INFO] topology.py:215 - HybridParallelInfo: rank_id: 1, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 6, mp_group: [1],  sharding_group: [1], pp_group: [1], dp_group: [0, 1, 2, 3, 4, 5], check/clip group: [1]
loading annotations into memory...
Done (t=1.61s)
creating index...
index created!
W0513 09:35:02.231353 25265 reducer.cc:622] All parameters are involved in the backward pass. It is recommended to set find_unused_parameters to False to improve performance. However, if unused parameters appear in subsequent iterative training, then an error will occur. Please make it clear that in the subsequent training, there will be no parameters that are not used in the backward pass, and then set find_unused_parameters
